---
title: "A2 A Practitionerâ€™s Guide to GLM"
author: "Anderson"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 4
---

## Cliff's Summary

Know the [advantages](#memorize1) of GLM over other methods (Especially one-way and CLM)

Be able to set up a the GLM matrices

Know the [3 components](#memorize2) of GLM (sys, response, link)

Know how to interpret the results

[Aliasing](#aliasing)

* Intrinsie = by definition
* Extrinsic = by nature of the data (correlated dependent variables)

Memorize:

* [Variance functions](#var-fun)
* [Link functions](#link-fun)
* [Common model](#com-model) forms

Calculations:

* Scale
* Deviance
* Type III test

## Types of Exam Questions

Shortcomings of methods compare to GLM

* 2003 Q25
* [2011 Q3b](#2011q3b)

Describe GLM and CLM assumptions and compare

* 2006 Q5 (GLM vs CLM)
* 2007 Q4
* 2009 Q3a

Aliasing

* 2008 Q3

Setting up equations, process

* [2009 Q3b](#2009q3b) (CLM)
* [2010 Q3c](#2010q3c) $\star$ (GLM)
* 2012 Q2 $\star$ (GLM)
* 2013 Q2 $\star$ (Solve GLM with identity link and normal variance)
    * Don't forget to weight them

Calculate results given parameters

* 2012 Q4a

Def of GLM parameters

* 2014 Q3a

Common Modeling form  

* 2014 Q3b
    * Not sure about why each of the dist^n^ are appropriate
    * The explaining weight part i'm not super sure as well

CLASSICAL LINEAR MODEL FITTING

* Setup $\Rightarrow$ Minimize SSE $\Rightarrow$ Minimize each $\beta$ by differentiate and set to 0

GENERALIZED LINEAR MODEL FITTING

* Setting a base means us it as the intercept

* $Y = g^{-1}(\beta_1 x_1 + \cdots + \beta_p x_p) + \epsilon$

* Solve by MLE with log likelihood

## Intro to GLM

**GLM**

Measure the relationship that a function of a linear combination of 1 or more explanatory variables has on a single dependent variable that is assumes to come from the exponential family of dist^n^

Generalized $\Rightarrow$ 

1. Linked function doesn't have to be the identity function

2. Dependent variable doesn't have to be normally distributed

Model: Pure Premium, frequency, severity, retention, loss development

## Comparison with other methods

***One-way analysis***

* Distortion by **correlations** among rating variables
* No consideration on **inter-dependencies** of rating variables on their impact on the dependent variable
    * Assumes no correlation between exposures for different rating variables

***

<a name="2011q3b"></a><span style="color:green">Good Question: 2011 Q3b</span>

* An example of a correlation would be if younger drivers were more likely to drive older cars. If a one-way analysis was being performed on age of vehicle, the older vehicles might look worse even though this might really relate to younger drivers.

* An example of an inter-dependency would be if 16 year old males have twice the claim frequency as 16 year old females, but 50 year old males have the same frequency as 50 year old females.

***

***Minimum bias***

<span style="color:green">Look it up later...</span>

* **No systematic test** on a particular variable's statistical significance
* **No credible range** for parameter estimates
* **Lack statistical framework** to assess quality
* Iterate so not as efficient
* (Some are special cases of GLM)
* From 2002 Feldblum Brosius

***Classical linear models***

* **Normality** and **constant variance** assumptions don't hold
    * Dependent variable might have restricted range;
    * Variance might be related to the mean
* **Relationships between predictors** might not be just additive

<a name="memorize1"></a>***GLM***  
<span style="color:red;background-color:yellow">Memorize</span>

* Statistical framework $\Rightarrow$ Explicit assumptions on the data and relationship with predictors
* More efficient than iterative methods
* Statistical diagnostics in selecting variables and validating model assumptions
* Adjust for correlations between variables and allows for interaction

## Classical Linear Models
$i = \text{observations}$  
$p = \text{variables}$

$Y \sim X$

* $Y$:Observed variable; $Y_i = \text{realization of r.v.} Y$
    
* $X$: covariates (predictors, levels of factors)

$Y = \mu + \epsilon = \beta_0 + \beta_1 \times x_1 + \cdots + \beta_p \times x_p + \epsilon = \mathrm{E[Y]} + \epsilon$  
$Y = X\beta + \epsilon$

$Y \sim N(\mu, \sigma^2)$  
$\epsilon \sim N(\mu, \sigma^2)$

## Solving CLM by hand  
<a name="2009q3b"></a><span style="color:green">Good Question: 2009 Q3b</span>

1. Setup equation
2. $Y_1 = \beta_0 + \sum\limits_{j=1}^p \beta_j x_j + \epsilon_1$  
$\vdots$  
$Y_n = \beta_0 + \sum\limits_{j=1}^p \beta_j x_j + \epsilon_n$
3. Solve for $\epsilon_i$
4. Plug in $\epsilon^2_i$ formulas; $SSE = \sum\limits_{i=1}^n \epsilon^2_i$
5. $\frac{\partial SSE}{\beta_0} = 0 \cdots \frac{\partial SSE}{\beta_p} = 0$
6. solve for $\beta$

## GLM vs CLM  
<a name="memorize2"></a><span style="color:red;background-color:yellow">Memorize</span>

### 1. Systematic component
**Same for both GLM and Classical Linear Model**  
Systematic component via a linear predictor

$\eta = \beta_1 x_1 + \cdots + \beta_p x_p$

### 2. Random component

**Classical Linear Model**  
Normal dist^n^ for response

$Y_i \text{'s are } i.i.d. \sim N(\mu_i, \sigma^2)$

$Var(Y_i) = \sigma^2$

**GLM**  
Exponential family for response

$Y_i$: i.i.d. from *exponential family* dist^n^

Variance function: $Var(Y_i) = \dfrac{\phi V(\mu_i)}{\omega_i}$

* $\phi$ is constant across all observations; scale the variance

* $\omega_i =$ prior weights; for each $i$

### 3. Link function

Connects the mean of the response to the linear predictor

$Y = g^{-1}(\eta) + \epsilon$

**Classical Linear Model**  
$E[Y] = \mu = \eta = \text{identify function}$
$g^{-1}(x) = x$

**GLM**  
Must be **differentiable** and **monotonic**  
$E[Y] = \mu = g^{-1}(\eta)$

## Common exponential family dist^n^

<span style="color:red;background-color:yellow">Memorize</span>
<a name="var-fun"></a>

| Error Distribution | Variance Function |
| ------------------ | ----------------- |
| Normal             | $V(x) = 1$        |
| Poisson            | $V(x) = x$        |
| Gamma              | $V(x) = x^2$      |
| Binomial           | $V(x) = x(1 - x)$; \# of trials = 1 |
| Inverse Gaussian   | $V(x) = x^3$      |
| Tweedie (Good for PP) | $V(x) = \frac{1}{\lambda}x^p$ where $p<0 \text{ or } 1 < p <2 \text{ or } p >2$ |

**Note**: 

* For Poisson and Gamma, higher variance for larger values $\Rightarrow$ less weight
    * Gamma gives even less weight to Poisson since the $V(x) = x^2$
* For Tweedie: $1 < p < 2$ is Poisson Gamma
* Side notes on [Tweedie](http://blog.revolutionanalytics.com/2014/10/a-note-on-tweedie.html)

## Prior weights

Use to give more weight to observations that should have more weight;

e.g. if one observation is a day's worth where another is a year's worth

For CLM, $SSE = \sum\limits_{i=1}^n \omega_i\epsilon^2_i$

## Scale Parameter

$\phi = 1$ for Poisson; else must be estimated from data along with $\beta$'s

Don't need $\phi$ to get the $\beta$'s; use for standard error

3 ways to estimate $\phi$:

1. MLE (not feasible)
2. Moment estimator (Pearson $\chi^2$):  
$\hat \phi = \frac{1}{n-p}\sum\limits_{i=1}^n \frac{\omega_i(Y_i - \mu_i)^2}{V(\mu_i)}$
3. Total deviance:  
$\hat \phi = \frac{D}{n-p}$

$n =$ \# of observations  
$p =$ \# of parameters  
$D =$ total deviance *(assume given if tested)*  
For CLM, $D = SSE$

***

The scale parameters allow the model to have a more flexible variance

* e.g. so your response doesn't have to actually have the mean = variance structure of a Poisson

## Common Link Functions  
<span style="color:red;background-color:yellow">Memorize</span>
<a name="link-fun"></a>

| Link Function | $g(\eta)$   | $g^{-1}(\eta)$ |
| ------------- | ----------- | -------------- |
| Identity      | $\eta$      | $\eta$         |
| Log           | $ln(\eta)$  | $e^{\eta}$     |
| Logit         | $ln(\frac{\eta}{1 - \eta})$ | $\frac{e^{\eta}}{1 + e^{\eta}}$ |
| Reciprocal    | $\frac{1}{\eta}$ | $\frac{1}{\eta}$ |

**Note**:  
Log is good for freq, sev; makes everything multiplicative  
Logit is good for retention as is has prob $0 < p < 1$

* Logit is basically the natural log odds
* Odd is $\frac{p}{1-p}$

## Offset Term

To fix the impact of an explanatory variable

* Used for covariate with *known* slope 

*Known* quantity for **each observation**

$\eta = \sum\limits_{j=1}^p \beta_j x_j + \xi$

Commonly use $\xi_i = ln(\text{exposure}_i)$ when modeling claim *counts* with a log link function  

Other source [explanation](http://stats.stackexchange.com/questions/11182/when-to-use-an-offset-in-a-poisson-regression)

## Common Model Forms  
<span style="color:red;background-color:yellow">Memorize</span>
<a name="com-model"></a>

**Claim frequencies/ counts**

* Multiplicative Poisson
* Log link; Poisson error

**Claim severity**

* Multiplicative Gamma
* Log link; Gamma error

**Pure Premium**

* Tweedie
* Compound Poisson and Gamma

**Probability**

* For policyholder retention
* Logistic
* Logit link; Binomial error

## Solving GLMs  
<a name="2010q3c"></a><span style="color:green">Good Question: 2010 Q3c</span>

Use MLE theoretically, numerical technique in practice  
ML set $\dfrac{\partial f}{\partial \beta_i} = 0$, solve  

Solve the normal equations or quasi-normal equations for maximum likelihood

## Interpreting Results

log link: $g^{-1}(\eta) = e^{\eta}$  
$E[Y] = exp(\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p)$  
if $Y = \text{Pure Premium} \Rightarrow e^{\beta_0} = \text{base rate}$; $e^{\beta_i x_i} = \text{rating factors}$

## Aliasing and near-aliasing  
<span style="color:red;background-color:yellow">Understand Concepts</span>
<a name="aliasing"></a>

Aliasing = linear dependency among covariates; model not uniquely defined

**Intrinsic Aliasing**

* Linear dependency occurs ***by definitions of the covariates***
    * Relationships in themselves don't cause intrinsic aliasing
    * Issue occurs because there are just as many parameters in the model as there are unique levels of the variables
* Solution: removing a parameter (or 2 parameter and add intercept)
* Make sure you have fewer $\beta$ than there are unique levels in you model

**Extrinsic Aliasing**

* Linear dependency occurs ***by nature of the data***
* Perfect correlation in your data
* Common with missing information across variables (confounding)
* Solution: remove a covariate

**Near Aliasing**

* Very high correlation in the data
* Solution: reclassify or delete problematic observations
* Can lead to convergence problem

## Testing Statistical Significance of Explanatory Variables
Feature selections

1. Size of CI
2. Type III testing
3. See if the parameter estimate is consistent over time
4. Intuitive that factor should impact result

### Type III Test
1. Create 2 models, one with the parameters to be tested the other without
2. Calculate degrees of freedom for each ($df = n - p$)
3. Calculate deviance *D* or scaled deviance $D^* = 
\frac{D}{\phi}$; use *D* if $\phi$ is unknown
    * *D* is usually given in exam or = to SSE in CLM
4. $\chi^2$ test if we used *D\**: $D^*_1 - D^*_2$  
*F* test if we used *D*: $\frac{D_1 - D_2}{(df_1 - df_2)/df_2}$
5. Compare to the appropriate dist^n^ with $(df_1 - df_2)$ *df*  
If test statistic is larger $\rightarrow$ additional parameters is statistically significant

***

If the model with additional parameter(s) has much lower SSE $\Rightarrow$ The parameters are significant

* We're judging how much lower the SSE needs to be with the test statistics that either follows a $\chi^2$ or *F* dist^n^